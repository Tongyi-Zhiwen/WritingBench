# WritingBench: A Comprehensive Benchmark for Generative Writing

## ðŸ“– Overview
WritingBench is a comprehensive benchmark for evaluating LLMs' writing capabilities across **1,239 real-world queries**, spanning:
- 6 primary domains 
- 100 fine-grained subdomains
- 3 core writing requirements: `Style` | `Format` | `Length`

WritingBench integrates diverse sources of materials, averaging 1,546 tokens per source. Each query is paired with 5 instance-specific criteria, scoring either through LLM evaluators or through a finetuned critic model.

![Comaparison](pics/comparision.pdf)

![Statistic](pics/statistic.pdf)

## ðŸ“Š Benchmark Construction

![Statistic](pics/construction.pdf)

## ðŸ“ˆ Evaluation Framework

![Statistic](pics/criteria.pdf)

## ðŸ›  Installation
```bash
git clone https://github.com/yourusername/WritingBench.git
```

## ðŸ“‚ Repository Structure
```bash
.
â”œâ”€â”€ evaluate_benchmark.py     # Evaluation script
â”œâ”€â”€ prompt.py                 # Prompt templates
â”œâ”€â”€ evaluator/
â”‚   â”œâ”€â”€ __int__.py
â”‚   â””â”€â”€ llm.py                # LLM evaluation interface
â””â”€â”€ benchmark_query/
    â”œâ”€â”€ benchmark_all.jsonl   # Full dataset (1239 queries)
    â””â”€â”€ requirement/
        â”œâ”€â”€ style/            # Style-specific subsets
        â”‚   â”œâ”€â”€ style_subset.jsonl
        â”‚   â””â”€â”€ style_subset_C.jsonl
        â”œâ”€â”€ format/           # Format-specific subsets
        â”‚   â”œâ”€â”€ format_subset.jsonl
        â”‚   â””â”€â”€ format_subset_C.jsonl
        â””â”€â”€ length/           # Length-specific subsets
            â”œâ”€â”€ length_subset.jsonl
            â””â”€â”€ length_subset_C.jsonl
```

## ðŸš€ Quick Start

1. Add your API credentials:
- For LLM-as-a-Judge, see evaluator/llm.py
```bash
  self.api_key = "your_api_key_here"
  self.url = "Your API endpoint"
  self.model = "Chose your model name"
```
- For critic model (coming soon)

2. Choose appropriate evaluation sets from benchmark_query/
```bash
python evaluate_benchmark.py \
  --query_criteria_file query_set.jsonl \
  --input_file samples.jsonl \
  --output_file scores.jsonl
```

## ðŸ“Š Benchmark Construction

ðŸ“œ Citation
